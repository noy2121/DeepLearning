{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eG0UIGa5YKvg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1768242000844,
     "user": {
      "displayName": "Noy Rahmani",
      "userId": "17221462539410065392"
     },
     "user_tz": -120
    },
    "id": "eG0UIGa5YKvg",
    "outputId": "0997f13e-87d0-418f-f53f-250670768717"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/Colab Notebooks/HW4\n"
     ]
    }
   ],
   "source": [
    "%cd '/content/HW4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7191768f",
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1768242367113,
     "user": {
      "displayName": "Noy Rahmani",
      "userId": "17221462539410065392"
     },
     "user_tz": -120
    },
    "id": "7191768f"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "from pathlib import Path\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from src.data_pipeline import CIFAR10Pipeline\n",
    "from src.runner import ExperimentRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b390120f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2986,
     "status": "ok",
     "timestamp": 1768242373880,
     "user": {
      "displayName": "Noy Rahmani",
      "userId": "17221462539410065392"
     },
     "user_tz": -120
    },
    "id": "b390120f",
    "outputId": "b491d222-9efc-4b8d-cedd-4c5ff6052f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "Setting up data pipeline...\n",
      "==============================\n",
      "\n",
      "Dataset: CIFAR-10\n",
      "Training samples: 45000\n",
      "Validation samples: 5000\n",
      "Test samples: 10000\n",
      "Batch size: 128\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load config\n",
    "\n",
    "cfg = OmegaConf.load(\"conf/config.yaml\")\n",
    "\n",
    "print(\"=\"*30)\n",
    "print(\"Setting up data pipeline...\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "data_pipeline = CIFAR10Pipeline(cfg)\n",
    "train_loader, val_loader, test_loader = data_pipeline.setup()\n",
    "\n",
    "print(f\"\\nDataset: CIFAR-10\")\n",
    "print(f\"Training samples: {len(train_loader.dataset)}\")\n",
    "print(f\"Validation samples: {len(val_loader.dataset)}\")\n",
    "print(f\"Test samples: {len(test_loader.dataset)}\")\n",
    "print(f\"Batch size: {cfg.train.batch_size}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "lzZGaLr4biyN",
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1768242374698,
     "user": {
      "displayName": "Noy Rahmani",
      "userId": "17221462539410065392"
     },
     "user_tz": -120
    },
    "id": "lzZGaLr4biyN"
   },
   "outputs": [],
   "source": [
    "# Auxiliary functions\n",
    "\n",
    "def print_exp_params(cfg, exp_name):\n",
    "\n",
    "  model_name = cfg.model.model_name\n",
    "\n",
    "  print(\"=\"*30)\n",
    "  print(f\"Experiment: {exp_name}\")\n",
    "  print(\"=\"*30)\n",
    "  print(f\"  Mode: {cfg.mode}\")\n",
    "  print(f\"  Model: {cfg.model.model_name}\")\n",
    "  print(f\"  Optimizer: {cfg.train.optimizer}\")\n",
    "  print(f\"  Learning Rate: {cfg.train.learning_rate}\")\n",
    "  print(f\"  Batch Size: {cfg.train.batch_size}\")\n",
    "  print(f\"  Epochs: {cfg.train.num_epochs}\")\n",
    "  print(f\"  Weight Decay: {cfg.train.weight_decay}\")\n",
    "\n",
    "  if model_name.lower() == 'customcnn':\n",
    "      print(f\"  CNN Layers: {cfg.model.cnn.num_layers}\")\n",
    "      print(f\"  Use BN: {cfg.model.cnn.use_batch_norm}\")\n",
    "      print(f\"  Use Dropout: {cfg.model.cnn.use_dropout}\")\n",
    "\n",
    "      if cfg.model.cnn.use_dropout:\n",
    "        print(f\"  Dropout Rate: {cfg.model.cnn.dropout_rate}\")\n",
    "\n",
    "      print(f\"  FC Layers: {cfg.model.cnn.fc_layers}\")\n",
    "\n",
    "\n",
    "def build_exp_name(exp_config):\n",
    "\n",
    "  model_name = exp_config['model']['model_name']\n",
    "  \n",
    "  if model_name.lower() == 'customcnn':\n",
    "    cnn = exp_config['model']['cnn']\n",
    "    train = exp_config['train']\n",
    "    \n",
    "    num_conv_layers = len(cnn['num_layers'])\n",
    "    arch_type = 'Shallow' if num_conv_layers <= 2 else 'Deep'\n",
    "    \n",
    "    conv_str = cnn['num_layers']\n",
    "    fc_str = cnn['fc_layers']\n",
    "    lr = train['learning_rate']\n",
    "    \n",
    "    # Build name\n",
    "    name = f\"{arch_type}_convs={conv_str}_fc={fc_str}_bz={train['batch_size']}_opt={train['optimizer']}_lr={lr}\"\n",
    "\n",
    "    return name\n",
    "  \n",
    "  # For ResNet models\n",
    "  else:\n",
    "    return model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a95c519",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1768242376897,
     "user": {
      "displayName": "Noy Rahmani",
      "userId": "17221462539410065392"
     },
     "user_tz": -120
    },
    "id": "0a95c519",
    "outputId": "2308b002-17e4-4ea5-a8ad-dbc0f1c50d8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "Experiment: ResNet18\n",
      "==============================\n",
      "  Mode: pretrained\n",
      "  Model: ResNet18\n",
      "  Optimizer: adam\n",
      "  Learning Rate: 0.001\n",
      "  Batch Size: 128\n",
      "  Epochs: 1\n",
      "  Weight Decay: 0.0001\n"
     ]
    }
   ],
   "source": [
    "# initialize experiment params\n",
    "\n",
    "cifar10_classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "                   'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# Dictionary to store results from all experiments\n",
    "experiment_results = {}\n",
    "\n",
    "# Run experiment for the configured model\n",
    "exp_name = build_exp_name(cfg)\n",
    "print_exp_params(cfg, exp_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27dff41",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 548
    },
    "executionInfo": {
     "elapsed": 61176,
     "status": "error",
     "timestamp": 1768242439989,
     "user": {
      "displayName": "Noy Rahmani",
      "userId": "17221462539410065392"
     },
     "user_tz": -120
    },
    "id": "a27dff41",
    "outputId": "797ec3e6-85c6-4013-b3b9-4c23a424da36"
   },
   "outputs": [],
   "source": [
    "# run single experiment\n",
    "\n",
    "# define experiment name as you wish\n",
    "exp_name = cfg.model.model_name\n",
    "\n",
    "runner = ExperimentRunner(\n",
    "    cfg=cfg,\n",
    "    exp_name=exp_name,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    class_names=cifar10_classes\n",
    ")\n",
    "\n",
    "results = runner.run()\n",
    "\n",
    "experiment_results[results['model_name']] = results\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"Experiment completed successfully!\")\n",
    "print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jzoldupuc7l",
   "metadata": {
    "id": "jzoldupuc7l"
   },
   "outputs": [],
   "source": [
    "# Load and evaluate existing model weights\n",
    "\n",
    "# Example: Load weights using config\n",
    "# Set cfg.load_weights = True and cfg.mode to match the model you want to load\n",
    "cfg.load_weights = True\n",
    "cfg.mode = 'pretrained'  # or 'scratch'\n",
    "cfg.train.freeze = False  # If loading pretrained model, set freeze accordingly\n",
    "\n",
    "# Create runner with load_weights=True\n",
    "runner = ExperimentRunner(\n",
    "    cfg=cfg,\n",
    "    exp_name=cfg.model.model_name,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    class_names=cifar10_classes,\n",
    "    load_weights=True\n",
    ")\n",
    "\n",
    "# Check if weights file exists\n",
    "print(f\"Model path: {runner.weights_path}\")\n",
    "print(f\"Model exists: {Path(runner.weights_path).exists()}\")\n",
    "\n",
    "# Run evaluation (will skip training and load weights)\n",
    "if Path(runner.weights_path).exists():\n",
    "    results = runner.run()\n",
    "    experiment_results[results['model_name']] = results\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Model evaluation completed!\")\n",
    "    print(\"=\"*60)\n",
    "else:\n",
    "    print(f\"\\nNo saved model found at {runner.weights_path}\")\n",
    "    print(\"Train a model first or check your config parameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fffa379",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "We perform the following experiments for each architecture (Shallow vs Deep)\n",
    "1. Architecture Params\n",
    "1. Batch Size (64 vs 128)\n",
    "2. Optimizer & lr (adam vs SGD, 1e-3 vs 1e-4)\n",
    "3. Regularization (Dropout vs BN vs Both)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27388ad5",
   "metadata": {},
   "source": [
    "### Exp1 - Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fd0698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define 1st experiment params\n",
    "experiments = [\n",
    "    \n",
    "    # Shallow CNN from scratch\n",
    "\n",
    "    # Single layer - very shallow\n",
    "    {\n",
    "        'mode': 'scratch',\n",
    "        'load_weights': False,\n",
    "        'model': {\n",
    "          'model_name': 'CustomCNN',\n",
    "          'cnn': {\n",
    "            'num_layers': [64],\n",
    "            'kernel_size': [3],\n",
    "            'stride': [1],\n",
    "            'fc_layers': [256],\n",
    "            'use_dropout': False,\n",
    "            'dropout_rate': 0.4,\n",
    "            'use_batch_norm': False\n",
    "          }\n",
    "        },\n",
    "        'train': {\n",
    "          'batch_size': 128,\n",
    "          'learning_rate': 0.001,\n",
    "          'num_epochs': 20,\n",
    "          'optimizer': 'adam',\n",
    "          'weight_decay': 0.0001,\n",
    "          'momentum': 0.9,\n",
    "          'early_stopping_patience': 3,\n",
    "          'freeze': 'false'\n",
    "        }\n",
    "    },\n",
    "\n",
    "    # Two layers - standard shallow\n",
    "    {\n",
    "        'mode': 'scratch',\n",
    "        'load_weights': False,\n",
    "        'model': {\n",
    "          'model_name': 'CustomCNN',\n",
    "          'cnn': {\n",
    "            'num_layers': [64, 128],\n",
    "            'kernel_size': [3, 3],\n",
    "            'stride': [1, 1],\n",
    "            'fc_layers': [256, 128],\n",
    "            'use_dropout': False,\n",
    "            'dropout_rate': 0.4,\n",
    "            'use_batch_norm': False\n",
    "          }\n",
    "        },\n",
    "        'train': {\n",
    "          'batch_size': 128,\n",
    "          'learning_rate': 0.001,\n",
    "          'num_epochs': 20,\n",
    "          'optimizer': 'adam',\n",
    "          'weight_decay': 0.0001,\n",
    "          'momentum': 0.9,\n",
    "          'early_stopping_patience': 3,\n",
    "          'freeze': 'false'\n",
    "        }\n",
    "    },\n",
    "\n",
    "    # Two layers - wider shallow (more channels)\n",
    "    {\n",
    "        'mode': 'scratch',\n",
    "        'load_weights': False,\n",
    "        'model': {\n",
    "          'model_name': 'CustomCNN',\n",
    "          'cnn': {\n",
    "            'num_layers': [128, 256],\n",
    "            'kernel_size': [3, 3],\n",
    "            'stride': [1, 1],\n",
    "          'fc_layers': [512],\n",
    "          'use_dropout': False,\n",
    "          'dropout_rate': 0.4,\n",
    "          'use_batch_norm': False\n",
    "          }\n",
    "        },\n",
    "        'train': {\n",
    "          'batch_size': 128,\n",
    "          'learning_rate': 0.001,\n",
    "          'num_epochs': 20,\n",
    "          'optimizer': 'adam',\n",
    "          'weight_decay': 0.0001,\n",
    "          'momentum': 0.9,\n",
    "          'early_stopping_patience': 3,\n",
    "          'freeze': False\n",
    "        }\n",
    "    },\n",
    "\n",
    "    # Deep CNN from scratch\n",
    "\n",
    "    # Four layers - standard deep\n",
    "    {\n",
    "        'mode': 'scratch',\n",
    "        'load_weights': False,\n",
    "        'model': {\n",
    "          'model_name': 'CustomCNN',\n",
    "          'cnn': {\n",
    "            'num_layers': [32, 64, 128, 256],\n",
    "            'kernel_size': [3, 3, 3, 3],\n",
    "            'stride': [1, 1, 1, 1],\n",
    "            'fc_layers': [512, 128],\n",
    "            'use_dropout': False,\n",
    "            'dropout_rate': 0.4,\n",
    "            'use_batch_norm': False\n",
    "          }\n",
    "        },\n",
    "        'train': {\n",
    "          'batch_size': 128,\n",
    "          'learning_rate': 0.001,\n",
    "          'num_epochs': 20,\n",
    "          'optimizer': 'adam',\n",
    "          'weight_decay': 0.0001,\n",
    "          'momentum': 0.9,\n",
    "          'early_stopping_patience': 3,\n",
    "          'freeze': False\n",
    "        }\n",
    "    },    \n",
    "\n",
    "    # Five layers - deeper\n",
    "    {\n",
    "        'mode': 'scratch',\n",
    "        'load_weights': False,\n",
    "        'model':\n",
    "        {\n",
    "          'model_name': 'CustomCNN',\n",
    "          'cnn': {\n",
    "            'num_layers': [32, 64, 64, 128, 256],\n",
    "            'kernel_size': [3, 3, 3, 3, 3],\n",
    "            'stride': [1, 1, 1, 1, 1],\n",
    "            'fc_layers': [512, 256, 128],\n",
    "            'use_dropout': False,\n",
    "            'dropout_rate': 0.4,\n",
    "            'use_batch_norm': False\n",
    "          }\n",
    "        },\n",
    "        'train': {\n",
    "          'batch_size': 128,\n",
    "          'learning_rate': 0.001,\n",
    "          'num_epochs': 20,\n",
    "          'optimizer': 'adam',\n",
    "          'weight_decay': 0.0001,\n",
    "          'momentum': 0.9,\n",
    "          'early_stopping_patience': 3,\n",
    "          'freeze': False\n",
    "        }\n",
    "    },\n",
    "\n",
    "    # Five layers - VGG-style (double channels progressively)\n",
    "    {\n",
    "        'mode': 'scratch',\n",
    "        'load_weights': False,\n",
    "        'model': {\n",
    "          'model_name': 'CustomCNN',\n",
    "          'cnn': {\n",
    "            'num_layers': [64, 128, 256, 512, 512],\n",
    "            'kernel_size': [3, 3, 3, 3, 3],\n",
    "            'stride': [1, 1, 1, 1, 1],\n",
    "            'fc_layers': [1024, 512],\n",
    "            'use_dropout': False,\n",
    "            'dropout_rate': 0.4,\n",
    "            'use_batch_norm': False\n",
    "          }\n",
    "        },\n",
    "        'train': {\n",
    "          'batch_size': 128,\n",
    "          'learning_rate': 0.001,\n",
    "          'num_epochs': 20,\n",
    "          'optimizer': 'adam',\n",
    "          'weight_decay': 0.0001,\n",
    "          'momentum': 0.9,\n",
    "          'early_stopping_patience': 3,\n",
    "          'freeze': False\n",
    "        }\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5feb93f",
   "metadata": {
    "id": "e5feb93f"
   },
   "outputs": [],
   "source": [
    "# run multiple experiments\n",
    "\n",
    "cfg.results_dir = 'results/Exp1_Architecture'\n",
    "cfg.checkpoints_dir = 'checkpoints/Exp1_Architecture'\n",
    "for exp in experiments:\n",
    "  \n",
    "  exp_name = build_exp_name(exp)\n",
    "\n",
    "  cfg.mode = exp['mode']\n",
    "  cfg.model.model_name = exp['model']['model_name']\n",
    "\n",
    "  if 'train' in exp:\n",
    "    for key in exp['train'].keys():\n",
    "      cfg['train'][key] = exp['train'][key]\n",
    "      \n",
    "  if 'cnn' in exp:\n",
    "    for key in exp['cnn'].keys():\n",
    "      cfg['model']['cnn'][key] = exp['cnn'][key]\n",
    "\n",
    "  # reload data for new config\n",
    "  data_pipeline = CIFAR10Pipeline(cfg)\n",
    "  train_loader, val_loader, test_loader = data_pipeline.setup()\n",
    "  \n",
    "  print_exp_params(cfg, exp_name)\n",
    "\n",
    "  runner = ExperimentRunner(\n",
    "      cfg=cfg,\n",
    "      exp_name=exp_name,\n",
    "      train_loader=train_loader,\n",
    "      val_loader=val_loader,\n",
    "      test_loader=test_loader,\n",
    "      class_names=cifar10_classes,\n",
    "      load_weights=exp['load_weights']\n",
    "  )\n",
    "\n",
    "  results = runner.run()\n",
    "  experiment_results[exp_name] = results\n",
    "\n",
    "# Compare all experiments\n",
    "ExperimentRunner.compare(experiment_results, save_dir=\"results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5942db7e",
   "metadata": {},
   "source": [
    "### Exp2 - Batch Size\n",
    "\n",
    "We took the best models from Exp1 for each network based on F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0106451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define 2nd experiment params\n",
    "experiments = [\n",
    "    \n",
    "    # Shallow CNN from scratch\n",
    "    {\n",
    "        'mode': 'scratch',\n",
    "        'load_weights': False,\n",
    "        'model': {\n",
    "          'model_name': 'CustomCNN',\n",
    "          'cnn': {\n",
    "            'num_layers': [64, 128],\n",
    "            'kernel_size': [3, 3],\n",
    "            'stride': [1, 1],\n",
    "            'fc_layers': [256, 128],\n",
    "            'use_dropout': False,\n",
    "            'dropout_rate': 0.4,\n",
    "            'use_batch_norm': False\n",
    "          }\n",
    "        },\n",
    "        'train': {\n",
    "          'batch_size': 32,\n",
    "          'learning_rate': 0.001,\n",
    "          'num_epochs': 20,\n",
    "          'optimizer': 'adam',\n",
    "          'weight_decay': 0.0001,\n",
    "          'momentum': 0.9,\n",
    "          'early_stopping_patience': 3,\n",
    "          'freeze': 'false'\n",
    "        }\n",
    "    },\n",
    "\n",
    "  \n",
    "    # Deep CNN from scratch\n",
    "    {\n",
    "        'mode': 'scratch',\n",
    "        'load_weights': False,\n",
    "        'model': {\n",
    "          'model_name': 'CustomCNN',\n",
    "          'cnn': {\n",
    "            'num_layers': [64, 128, 256, 512, 512],\n",
    "            'kernel_size': [3, 3, 3, 3, 3],\n",
    "            'stride': [1, 1, 1, 1, 1],\n",
    "            'fc_layers': [1024, 512],\n",
    "            'use_dropout': False,\n",
    "            'dropout_rate': 0.4,\n",
    "            'use_batch_norm': False\n",
    "          }\n",
    "        },\n",
    "        'train': {\n",
    "          'batch_size': 128,\n",
    "          'learning_rate': 0.001,\n",
    "          'num_epochs': 20,\n",
    "          'optimizer': 'adam',\n",
    "          'weight_decay': 0.0001,\n",
    "          'momentum': 0.9,\n",
    "          'early_stopping_patience': 3,\n",
    "          'freeze': False\n",
    "        }\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba37f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run multiple experiments\n",
    "\n",
    "cfg.results_dir = 'results/Exp2_BatchSize'\n",
    "cfg.checkpoints_dir = 'checkpoints/Exp2_BatchSize'\n",
    "for exp in experiments:\n",
    "\n",
    "  for bz in [32, 64, 128, 256]:\n",
    "\n",
    "    exp['train']['batch_size'] = bz\n",
    "    cfg.train.batch_size = bz\n",
    "    exp_name = build_exp_name(exp)\n",
    "\n",
    "    cfg.mode = exp['mode']\n",
    "    cfg.model.model_name = exp['model']['model_name']\n",
    "\n",
    "    if 'train' in exp:\n",
    "      for key in exp['train'].keys():\n",
    "        cfg['train'][key] = exp['train'][key]\n",
    "        \n",
    "    if 'cnn' in exp:\n",
    "      for key in exp['cnn'].keys():\n",
    "        cfg['model']['cnn'][key] = exp['cnn'][key]\n",
    "\n",
    "    # reload data for new config\n",
    "    data_pipeline = CIFAR10Pipeline(cfg)\n",
    "    train_loader, val_loader, test_loader = data_pipeline.setup()\n",
    "    \n",
    "    print_exp_params(cfg, exp_name)\n",
    "\n",
    "    runner = ExperimentRunner(\n",
    "        cfg=cfg,\n",
    "        exp_name=exp_name,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,\n",
    "        class_names=cifar10_classes,\n",
    "        load_weights=exp['load_weights']\n",
    "    )\n",
    "\n",
    "    results = runner.run()\n",
    "    experiment_results[exp_name] = results\n",
    "\n",
    "# Compare all experiments\n",
    "ExperimentRunner.compare(experiment_results, save_dir=\"results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d271b0",
   "metadata": {},
   "source": [
    "### Exp3 - Optimizer\n",
    "\n",
    "Batch sizes 32 and 64 gave the best results. We will test the optimizers with either one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4355384b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define 3nd experiment params\n",
    "experiments = [\n",
    "    \n",
    "    # Shallow CNN from scratch\n",
    "    {\n",
    "        'mode': 'scratch',\n",
    "        'load_weights': False,\n",
    "        'model': {\n",
    "          'model_name': 'CustomCNN',\n",
    "          'cnn': {\n",
    "            'num_layers': [64, 128],\n",
    "            'kernel_size': [3, 3],\n",
    "            'stride': [1, 1],\n",
    "            'fc_layers': [256, 128],\n",
    "            'use_dropout': False,\n",
    "            'dropout_rate': 0.4,\n",
    "            'use_batch_norm': False\n",
    "          }\n",
    "        },\n",
    "        'train': {\n",
    "          'batch_size': 32,\n",
    "          'learning_rate': 0.001,\n",
    "          'num_epochs': 20,\n",
    "          'optimizer': 'adam',\n",
    "          'weight_decay': 0.0001,\n",
    "          'momentum': 0.9,\n",
    "          'early_stopping_patience': 3,\n",
    "          'freeze': 'false'\n",
    "        }\n",
    "    },\n",
    "\n",
    "  \n",
    "    # Deep CNN from scratch\n",
    "    {\n",
    "        'mode': 'scratch',\n",
    "        'load_weights': False,\n",
    "        'model': {\n",
    "          'model_name': 'CustomCNN',\n",
    "          'cnn': {\n",
    "            'num_layers': [64, 128, 256, 512, 512],\n",
    "            'kernel_size': [3, 3, 3, 3, 3],\n",
    "            'stride': [1, 1, 1, 1, 1],\n",
    "            'fc_layers': [1024, 512],\n",
    "            'use_dropout': False,\n",
    "            'dropout_rate': 0.4,\n",
    "            'use_batch_norm': False\n",
    "          }\n",
    "        },\n",
    "        'train': {\n",
    "          'batch_size': 128,\n",
    "          'learning_rate': 0.001,\n",
    "          'num_epochs': 20,\n",
    "          'optimizer': 'adam',\n",
    "          'weight_decay': 0.0001,\n",
    "          'momentum': 0.9,\n",
    "          'early_stopping_patience': 3,\n",
    "          'freeze': False\n",
    "        }\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3d87cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "Experiment: Shallow_convs=[64, 128]_fc=[256, 128]_bz=32_opt=adam_lr=0.001\n",
      "==============================\n",
      "  Mode: scratch\n",
      "  Model: CustomCNN\n",
      "  Optimizer: adam\n",
      "  Learning Rate: 0.001\n",
      "  Batch Size: 32\n",
      "  Epochs: 20\n",
      "  Weight Decay: 0.0001\n",
      "  CNN Layers: [32, 64]\n",
      "  Use BN: True\n",
      "  Use Dropout: True\n",
      "  Dropout Rate: 0.3\n",
      "  FC Layers: [128]\n",
      "\n",
      "Initializing CustomCNN...\n",
      "Mode: scratch\n",
      "Built CustomCNN with 29,194 parameters\n",
      "\n",
      "Training for 20 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20] Train Loss: 1.9554 Acc: 25.46% | Val Loss: 1.7206 Acc: 36.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20] Train Loss: 1.8291 Acc: 30.61% | Val Loss: 1.6579 Acc: 36.68%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20] Train Loss: 1.7861 Acc: 32.27% | Val Loss: 1.6048 Acc: 39.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20] Train Loss: 1.7435 Acc: 34.52% | Val Loss: 1.5831 Acc: 42.08%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20] Train Loss: 1.7118 Acc: 36.12% | Val Loss: 1.5449 Acc: 43.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20] Train Loss: 1.6767 Acc: 37.56% | Val Loss: 1.5032 Acc: 45.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 49\u001b[39m\n\u001b[32m     37\u001b[39m         print_exp_params(cfg, exp_name)\n\u001b[32m     39\u001b[39m         runner = ExperimentRunner(\n\u001b[32m     40\u001b[39m             cfg=cfg,\n\u001b[32m     41\u001b[39m             exp_name=exp_name,\n\u001b[32m   (...)\u001b[39m\u001b[32m     46\u001b[39m             load_weights=exp[\u001b[33m'\u001b[39m\u001b[33mload_weights\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     47\u001b[39m         )\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m         results = \u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m         experiment_results[exp_name] = results\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# Compare all experiments\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Noy\\Code\\Noy\\DeepLearning\\HW4\\src\\runner.py:150\u001b[39m, in \u001b[36mExperimentRunner.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    148\u001b[39m     \u001b[38;5;28mself\u001b[39m.load_model_weights()\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.evaluate()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Noy\\Code\\Noy\\DeepLearning\\HW4\\src\\runner.py:86\u001b[39m, in \u001b[36mExperimentRunner.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     83\u001b[39m early_stopping_patience = \u001b[38;5;28mself\u001b[39m.cfg.train.early_stopping_patience\n\u001b[32m     84\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTraining for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.cfg.train.num_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m epochs...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclassifier\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_patience\u001b[49m\u001b[43m=\u001b[49m\u001b[43mearly_stopping_patience\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[38;5;66;03m# Save model weights\u001b[39;00m\n\u001b[32m     94\u001b[39m \u001b[38;5;28mself\u001b[39m.classifier.save_model(\u001b[38;5;28mself\u001b[39m.weights_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Noy\\Code\\Noy\\DeepLearning\\HW4\\src\\classifier.py:156\u001b[39m, in \u001b[36mClassifier.train\u001b[39m\u001b[34m(self, train_loader, val_loader, num_epochs, early_stopping_patience)\u001b[39m\n\u001b[32m    153\u001b[39m patience_counter = \u001b[32m0\u001b[39m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     train_loss, train_acc = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m     val_loss, val_acc = \u001b[38;5;28mself\u001b[39m.evaluate(val_loader)\n\u001b[32m    159\u001b[39m     \u001b[38;5;28mself\u001b[39m.train_losses.append(train_loss)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Noy\\Code\\Noy\\DeepLearning\\HW4\\src\\classifier.py:102\u001b[39m, in \u001b[36mClassifier.train_epoch\u001b[39m\u001b[34m(self, train_loader)\u001b[39m\n\u001b[32m     99\u001b[39m total = \u001b[32m0\u001b[39m\n\u001b[32m    101\u001b[39m pbar = tqdm(train_loader, desc=\u001b[33m'\u001b[39m\u001b[33mTraining\u001b[39m\u001b[33m'\u001b[39m, leave=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Noy\\Code\\Noy\\DeepLearning\\.venv\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Noy\\Code\\Noy\\DeepLearning\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Noy\\Code\\Noy\\DeepLearning\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:788\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    787\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    790\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Noy\\Code\\Noy\\DeepLearning\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_collation:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset, \u001b[33m\"\u001b[39m\u001b[33m__getitems__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     52\u001b[39m         data = [\u001b[38;5;28mself\u001b[39m.dataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Noy\\Code\\Noy\\DeepLearning\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:416\u001b[39m, in \u001b[36mSubset.__getitems__\u001b[39m\u001b[34m(self, indices)\u001b[39m\n\u001b[32m    414\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__([\u001b[38;5;28mself\u001b[39m.indices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    415\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Noy\\Code\\Noy\\DeepLearning\\.venv\\Lib\\site-packages\\torchvision\\datasets\\cifar.py:119\u001b[39m, in \u001b[36mCIFAR10.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    116\u001b[39m img = Image.fromarray(img)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     img = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.target_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    122\u001b[39m     target = \u001b[38;5;28mself\u001b[39m.target_transform(target)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Noy\\Code\\Noy\\DeepLearning\\.venv\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Noy\\Code\\Noy\\DeepLearning\\.venv\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[39m, in \u001b[36mToTensor.__call__\u001b[39m\u001b[34m(self, pic)\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[32m    130\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    131\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    135\u001b[39m \u001b[33;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[32m    136\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Noy\\Code\\Noy\\DeepLearning\\.venv\\Lib\\site-packages\\torchvision\\transforms\\functional.py:168\u001b[39m, in \u001b[36mto_tensor\u001b[39m\u001b[34m(pic)\u001b[39m\n\u001b[32m    166\u001b[39m \u001b[38;5;66;03m# handle PIL Image\u001b[39;00m\n\u001b[32m    167\u001b[39m mode_to_nptype = {\u001b[33m\"\u001b[39m\u001b[33mI\u001b[39m\u001b[33m\"\u001b[39m: np.int32, \u001b[33m\"\u001b[39m\u001b[33mI;16\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sys.byteorder == \u001b[33m\"\u001b[39m\u001b[33mlittle\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mI;16B\u001b[39m\u001b[33m\"\u001b[39m: np.int16, \u001b[33m\"\u001b[39m\u001b[33mF\u001b[39m\u001b[33m\"\u001b[39m: np.float32}\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m img = torch.from_numpy(\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_to_nptype\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43muint8\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pic.mode == \u001b[33m\"\u001b[39m\u001b[33m1\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    171\u001b[39m     img = \u001b[32m255\u001b[39m * img\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# run multiple experiments\n",
    "\n",
    "cfg.results_dir = 'results/Exp3_Optimizer'\n",
    "cfg.checkpoints_dir = 'checkpoints/Exp3_Optimizer'\n",
    "for exp in experiments:\n",
    "\n",
    "  for bz in [32, 64]:\n",
    "    \n",
    "    for opt in ['adam', 'sgd']:\n",
    "      \n",
    "      for lr in [0.001, 0.0001]:\n",
    "\n",
    "        exp['train']['batch_size'] = bz\n",
    "        exp['train']['optimizer'] = opt\n",
    "        exp['train']['learning_rate'] = lr\n",
    "\n",
    "        cfg.train.batch_size = bz\n",
    "        cfg.train.optimizer = opt\n",
    "        cfg.train.learning_rate = lr\n",
    "        exp_name = build_exp_name(exp)\n",
    "\n",
    "        cfg.mode = exp['mode']\n",
    "        cfg.model.model_name = exp['model']['model_name']\n",
    "\n",
    "        if 'train' in exp:\n",
    "          for key in exp['train'].keys():\n",
    "            cfg['train'][key] = exp['train'][key]\n",
    "            \n",
    "        if 'cnn' in exp:\n",
    "          for key in exp['cnn'].keys():\n",
    "            cfg['model']['cnn'][key] = exp['cnn'][key]\n",
    "\n",
    "        # reload data for new config\n",
    "        data_pipeline = CIFAR10Pipeline(cfg)\n",
    "        train_loader, val_loader, test_loader = data_pipeline.setup()\n",
    "        \n",
    "        print_exp_params(cfg, exp_name)\n",
    "\n",
    "        runner = ExperimentRunner(\n",
    "            cfg=cfg,\n",
    "            exp_name=exp_name,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            test_loader=test_loader,\n",
    "            class_names=cifar10_classes,\n",
    "            load_weights=exp['load_weights']\n",
    "        )\n",
    "\n",
    "        results = runner.run()\n",
    "        experiment_results[exp_name] = results\n",
    "\n",
    "# Compare all experiments\n",
    "ExperimentRunner.compare(experiment_results, save_dir=\"results\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
